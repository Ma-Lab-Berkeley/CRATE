<!doctype html>
<html>
    <head>
        <!-- Google tag (gtag.js) -->
        <meta charset="utf-8" />
        <title>White-Box Transformers via Sparse Rate Reduction</title>

        <meta content="" name="description" />
        <meta
            content="White-Box Transformers via Sparse Rate Reduction"
            property="og:title"
        />
        <meta
            content="CRATE is a transformer-like architecture which is constructed through first principles and has competitive performance on standard tasks while also enjoying many side benefits."
            property="og:description"
        />
        <meta
            content="https://ma-lab-berkeley.github.io/CRATE/assets/card.png"
            property="og:image"
        />
        <meta
            content="White-Box Transformers via Sparse Rate Reduction"
            property="twitter:title"
        />
        <meta
            content="CRATE is a transformer-like architecture which is constructed through first principles and has competitive performance on standard tasks while also enjoying many side benefits."
            property="twitter:description"
        />
        <meta
            content="https://ma-lab-berkeley.github.io/CRATE/assets/card.png"
            property="twitter:image"
        />
        <meta property="og:type" content="website" />
        <meta content="summary_large_image" name="twitter:card" />
        <meta
            name="viewport"
            content="width=device-width, initial-scale=1, minimum-scale=1"
        />

        <link href="/CRATE/assets/favicon.png" rel="shortcut icon" type="image/x-icon" />

        <link href="https://fonts.googleapis.com" rel="preconnect" />
        <link
            href="https://fonts.gstatic.com"
            rel="preconnect"
            crossorigin="anonymous"
        />
        <script
        src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"
        type="text/javascript"
    ></script>
        <script type="text/javascript">
        WebFont.load({
            google: {
                families: [
                    "Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic",
                ],
            },
        });
        </script>
        <!--[if lt IE 9
]><script
src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"
type="text/javascript"
></script
><![endif]-->

        <script
        src="https://code.jquery.com/jquery-3.7.0.slim.min.js"
        integrity="sha256-tG5mcZUtJsZvyKAxYLVXrmjKBVLd6VpVccqz/r4ypFE="
        crossorigin="anonymous"
    ></script>

        <!-- Image compare utility. -->
        <!--<link href="./image-compare.css" rel="stylesheet" type="text/css" />
<script src="./image-compare.js" type="text/javascript"></script>-->

        <link
            href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css"
            rel="stylesheet"
            type="text/css"
        />
        <link
            rel="stylesheet"
            href="https://cdn.jsdelivr.net/npm/@tabler/icons@latest/iconfont/tabler-icons.min.css"
        />
        <link href="style.css" rel="stylesheet" type="text/css" />

        <!-- KaTeX -->
        <link
            rel="stylesheet"
            href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"
            integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn"
            crossorigin="anonymous"
        />
        <script
        defer
        src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"
        integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx"
        crossorigin="anonymous"
    ></script>
        <script
        defer
        src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
        integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
        crossorigin="anonymous"
    ></script>
        <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    { left: "$$", right: "$$", display: true },
                    { left: "$", right: "$", display: false },
                    { left: "\\(", right: "\\)", display: false },
                    { left: "\\[", right: "\\]", display: true },
                ],
                // • rendering keys, e.g.:
                throwOnError: false,
            });
        });
        </script>

        <script type="text/javascript">
        function toggleBibtex(bibtex_id) {
            const $bibtex = $(bibtex_id);
            $bibtex.css(
                "display",
                $bibtex.css("display") === "none" ? "block" : "none",
            );
        }
        </script>
    </head>

    <body>
        <h1 style="margin: 0.25em 0">
            White-Box Transformers via Sparse Rate Reduction: Compression is All There Is?
        </h1>

        <div style="text-align: center; font-weight: 600">@ NeurIPS 2023, CPAL 2024, ICLR 2024</div>
        <div class="title-spacer"></div>

        <!-- Authors -->
        <div
            id="authors-div",
            style="
            display: flex;
            flex-wrap: wrap;
            text-align: center;
            text-wrap: balance;
            justify-content: center;
            max-width: 40em;
            margin: 0 auto;
            font-weight: 600;
            color: #666;
            "
        >
            <!-- We group authors based on how we want them to wrap. Sam and Yi are
grouped together, etc. -->
            <div>
                <a href="https://yaodongyu.github.io/" target="_blank">Yaodong Yu</a><sup>1</sup>
                <div class="name-spacer", style="display: inline-block"></div>
                <a href="https://sdbuchanan.com/" target="_blank">Sam Buchanan</a><sup>2</sup>
                <div class="name-spacer", style="display: inline-block"></div>
                <a href="https://druvpai.github.io/" target="_blank">Druv Pai</a><sup>1</sup>
            </div>

            <div>
                <a href="https://tianzhechu.com/" target="_blank">Tianzhe Chu</a><sup>1 3</sup>
                <div class="name-spacer", style="display: inline-block"></div>
                <a href="https://robinwu218.github.io/" target="_blank">Ziyang Wu</a><sup>1</sup>
                <div class="name-spacer", style="display: inline-block"></div>
                <a href="https://tsb0601.github.io/petertongsb/" target="_blank">Shengbang Tong</a><sup>1</sup>
                <div class="name-spacer", style="display: inline-block"></div>
                <a href="http://www.jackgethome.com/" target="_blank">Hao Bai</a><sup>4</sup>
                <div class="name-spacer", style="display: inline-block"></div>
                <a href="https://yx-s-z.github.io/" target="_blank">Yuexiang Zhai</a><sup>1</sup>
            </div>

            <div>
                <a href="https://www.cis.jhu.edu/~haeffele/" target="_blank">Ben Haeffele</a><sup>5</sup>
                <div class="name-spacer", style="display: inline-block"></div>
                <a href="https://people.eecs.berkeley.edu/~yima/" target="_blank">Yi Ma</a><sup>1 6</sup>
            </div>
        </div>
        <div class="title-spacer"></div>

        <!-- Affiliations -->
        <div
            style="
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 2em;
            max-width: 30em;
            margin: 0 auto;
            font-weight: 400;
            text-wrap: balance;
            text-align: center;
            "
        >
            <div id="affiliations-list">
                <sup>1</sup>UC Berkeley&nbsp;&nbsp;
                <sup>2</sup>TTIC&nbsp;&nbsp;
                <sup>3</sup>ShanghaiTech&nbsp;&nbsp;
                <sup>4</sup>UIUC&nbsp;&nbsp;
                <sup>5</sup>JHU&nbsp;&nbsp;
                <sup>6</sup>HKU&nbsp;&nbsp;
            </div>
        </div>
        <div class="title-spacer"></div>

        <!-- Links -->
        <div class="button_list">
            <a href="https://github.com/Ma-Lab-Berkeley/CRATE" target="_blank">
                <button>
                    <i class="ti ti-brand-github"></i>
                    GitHub
                </button>
            </a>
            <a href="https://arxiv.org/abs/2311.13110" target="_blank">
                <button>
                    <i class="ti ti-article"></i>
                    arXiv
                </button>
            </a>
            <a onclick="toggleBibtex('#bibtex_journal_paper')">
                <button>
                    <i class="ti ti-book-2"></i>
                    BibTeX
                </button>
            </a>
            <a href="https://drive.google.com/drive/folders/1j7wtXteUA0dNT8Bl5Q1hVGpBU9_QAzKu" target="_blank">
                <button>
                    <i class="ti ti-presentation"></i>
                    Tutorial
                </button>
            </a>
        </div>

        <section id="bibtex_journal_paper" style="display: none">
            <div style="height: 1em"></div>
            <div style="height: 1em"></div>
            <p>
                <code>@misc{yu2023whitebox,
    title={
        White-Box Transformers via Sparse Rate Reduction:
        Compression Is All There Is?
    },
    author={
        Yaodong Yu and Sam Buchanan and Druv Pai
        and Tianzhe Chu and Ziyang Wu and Shengbang Tong and Hao Bai
        and Yuexiang Zhai and Benjamin D. Haeffele and Yi Ma
    },
    year={2023},
    eprint={2311.13110},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}</code>
            </p>
        </section>
        <div class="figure-spacer"></div>

        <!-- Big figure -->
        <img src="./assets/CRATE_pipeline.svg" class="big-figure" />
        <div class="figure-spacer"></div>

        <!-- tldr -->
        <section>
            <h2>Summary</h2>

            <p>
                <em><b>CRATE</b> is a transformer-like architecture which is constructed through first principles, enjoys a rich theoretical framework, and achieves competitive performance across diverse training setups.</em>
            </p>

            <p>
                At the top of the page, we have linked a long-form manuscript explaining the CRATE architecture in full detail. Below, we summarize the numerous sub-projects that have developed the CRATE architecture. These consist of:
            </p>

            <ul>
                <li><a href="#crate_original_paper" class="internal_link">Our NeurIPS 2023 paper</a>, which developed the original CRATE framework and applied it to vision classification.</li>
                <li><a href="#crate_seg_paper" class="internal_link">Our CPAL 2024 paper</a>, which used the learned representations in classification-trained CRATE models for semantic segmentation tasks.</li>
                <li><a href="#crate_ae_paper" class="internal_link">Our ICLR 2024 paper</a>, which extended the CRATE framework to self-supervised learning tasks, including (masked) autoencoding.</li>
            </ul>

            <p>
                Code, paper links, bibliographic information, and short write-ups are listed below for each project. At the <a href="#follow-up-projects" class="internal_link">bottom of the page</a>, we also shout-out some follow-up work.
            </p>
        </section>

        <div style="height: 1em"></div>

        <section id="crate_original_paper">
            <h1>White-Box Transformers via Sparse Rate Reduction</h1>
            <h2>@ NeurIPS 2023</h2>

            <!-- Links -->
            <div class="button_list">
                <a href="https://github.com/Ma-Lab-Berkeley/CRATE" target="_blank">
                    <button>
                        <i class="ti ti-brand-github"></i>
                        GitHub
                    </button>
                </a>
                <a href="https://arxiv.org/abs/2306.01129" target="_blank">
                    <button>
                        <i class="ti ti-article"></i>
                        arXiv
                    </button>
                </a>
                <a onclick="toggleBibtex('#bibtex_neurips_paper')">
                    <button>
                        <i class="ti ti-book-2"></i>
                        BibTeX
                    </button>
                </a>
            </div>

            <section id="bibtex_neurips_paper" style="display: none">
                <div style="height: 1em"></div>
                <div style="height: 1em"></div>
                <pre>
                    <code>@article{yu2024white,
    title={White-Box Transformers via Sparse Rate Reduction},
    author={
        Yu, Yaodong and Buchanan, Sam and Pai, Druv
        and Chu, Tianzhe and Wu, Ziyang and Tong, Shengbang
        and Haeffele, Benjamin and Ma, Yi
    },
    journal={Advances in Neural Information Processing Systems},
    volume={36},
    year={2024}
}</code>
                </pre>
            </section>
            <div style="height: 1em"></div>

            <img src="./assets/CRATE_encoding_pipeline.svg" class="reg-figure" />
            <div style="height: 1em"></div>


            <h2>What is CRATE?</h2>
            <p>
                <b>CRATE</b> (<b>C</b>oding <b>RATE</b> transformer) is a white-box (mathematically interpretable) transformer architecture, where each layer performs a single step of an alternating minimization algorithm to optimize the sparse rate reduction objective
            </p>
            <div id="srr-div">
                \[\max_{f}\mathbb{E}_{\boldsymbol{Z} = f(\boldsymbol{X})}[\Delta R(\boldsymbol{Z} \mid \boldsymbol{U}_{[K]}) - \lambda \|\boldsymbol{Z}\|_{0}] = \max_{f}\mathbb{E}_{\boldsymbol{Z} = f(\boldsymbol{X})}[R(\boldsymbol{Z}) - R^{c}(\boldsymbol{Z} \mid \boldsymbol{U}_{[K]}) - \lambda \|\boldsymbol{Z}\|_{0}],\]
            </div>
            <div id="srr-div-mobile">
                \[
                \begin{aligned}
                &\max_{f}\mathbb{E}_{\boldsymbol{Z} = f(\boldsymbol{X})}[\Delta R(\boldsymbol{Z} \mid \boldsymbol{U}_{[K]}) - \lambda
                \|\boldsymbol{Z}\|_{0}]
                \\ =& \max_{f}\mathbb{E}_{\boldsymbol{Z} = f(\boldsymbol{X})}[R(\boldsymbol{Z}) - R^{c}(\boldsymbol{Z} \mid
                \boldsymbol{U}_{[K]}) - \lambda \|\boldsymbol{Z}\|_{0}],
                \end{aligned}
                \]
            </div>
            <p>
                where \(R\) and \(R^{c}\) are measures of compression of the final token representations \(\boldsymbol{Z} = f(\boldsymbol{X})\) w.r.t. different codebooks, and the \(\ell^{0}\) norm promotes the sparsity of \(\boldsymbol{Z}\). Overall, the sparse rate reduction objective promotes compact and sparse representations.
            </p>
            <p>
                The function \(f\) is defined as
            </p>
                \[f = f^{L} \circ f^{L - 1} \circ \cdots \circ f^{1} \circ f^{\mathrm{pre}},\]
            <p>
                where \(f^{\mathrm{pre}}\) is the pre-processing mapping, and \(f^{\ell}\) is the \(\ell^{\mathrm{th}}\)-layer forward mapping that transforms the token distribution to optimize the above sparse rate reduction objective incrementally.
            </p>
            <img src="./assets/sparse_rate_reduction.svg" class="narrow-figure"/>
            <p>
                In particular, \(f^{\ell}\) implements a step of a parameterized optimization algorithm on the sparse rate reduction, thus making the representations more compact and sparse.
            </p>

            <h2>Architecture</h2>

            <p>
                The following figure presents an overview of the general CRATE architecture:
            </p>
            <img src="./assets/CRATE_pipeline.svg" class="reg-figure"/>
            <p>
                After encoding input data \(\boldsymbol{X}\) as a sequence of tokens \(\boldsymbol{Z}^1\), CRATE constructs a deep network that transforms the data to a canonical configuration of low-dimensional subspaces by successive <it style="color:#00BFFF">compression</it> against a local model for the distribution, generating \(\boldsymbol{Z}^{\ell+1/2}\), and <it style="color:#008000">sparsification</it> against a global dictionary, generating \(\boldsymbol{Z}^{\ell+1}\). Repeatedly stacking these blocks and training the model parameters via backpropagation yields a powerful and interpretable representation of the data.
            </p>
            <img src="./assets/CRATE_layer.svg" class="reg-figure"/>
            <p>
                The full architecture is simply a concatenation of such layers, with some initial tokenizer and final task-specific architecture (i.e., a classification head). <b>Overall, CRATE is similar to a transformer, with two differences:</b>
                <ul>
                    <li>in each attention head, the \(\boldsymbol{Q}\), \(\boldsymbol{K}\), and \(\boldsymbol{V}\) weight matrices are weight-tied, i.e., set to be equal;</li>
                    <li>and the nonlinearity following each attention layer is no longer a multi-layer perceptron (MLP), but rather a more structured operator (\(\texttt{ISTA}\)) with sparse outputs.</li>
                </ul>
            </p>

            <h2>Classification</h2>
            <p>
                Below, the classification pipeline for CRATE is depicted. It is virtually identical to the popular vision transformer.
            </p>
            <img src="./assets/CRATE_classification.svg" class="reg-figure"/>
            <p>
                We use soft-max cross entropy loss to train on the supervised image classification task. We obtain competitive performance with the usual vision transformer (ViT) trained on classification, with similar scaling behavior, including  <b>above 80% top-1 accuracy on ImageNet-1K with 25% of the parameters of ViT</b>.
            </p>
        </section>

        <section id="crate_seg_paper">
            <h1>Emergence of Segmentation with Minimalistic White-Box Transformers</h1>
            <h2>@ CPAL 2024</h2>
            <!-- Links -->
            <div class="button_list">
                <a href="https://github.com/Ma-Lab-Berkeley/CRATE" target="_blank">
                    <button>
                        <i class="ti ti-brand-github"></i>
                        GitHub
                    </button>
                </a>
                <a href="https://arxiv.org/abs/2308.16271" target="_blank">
                    <button>
                        <i class="ti ti-article"></i>
                        arXiv
                    </button>
                </a>
                <a onclick="toggleBibtex('#bibtex_cpal_paper')">
                    <button>
                        <i class="ti ti-book-2"></i>
                        BibTeX
                    </button>
                </a>
                <a href="https://colab.research.google.com/drive/1rYn_NlepyW7Fu5LDliyBDmFZylHco7ss?usp=sharing" target="_blank">
                    <button>
                        <i class="ti ti-pencil"></i>
                        Colab Demo
                    </button>
                </a>
            </div>

            <section id="bibtex_cpal_paper" style="display: none">
                <div style="height: 1em"></div>
                <div style="height: 1em"></div>
                <pre>
                    <code>@inproceedings{yu2024emergence,
    title={
        Emergence of Segmentation with Minimalistic White-Box Transformers
    },
    author={
        Yu, Yaodong and Chu, Tianzhe and Tong, Shengbang and Wu, Ziyang
        and Pai, Druv and Buchanan, Sam and Ma, Yi
    },
    booktitle={Conference on Parsimony and Learning},
    pages={72--93},
    year={2024},
    organization={PMLR}
}</code>
                </pre>
            </section>
            <div style="height: 1em"></div>

            <h2>Segmentation and Object Detection</h2>

            <p>
                An interesting phenomenon of CRATE is that <i>even when trained on supervised classification</i>, it <i>learns to segment</i> the input images, with such segmentations being easily recoverable via attention maps, as in the following pipeline (similar to DINO).
            </p>
            <img src="./assets/CRATE_attention.svg" class="reg-figure"/>
            <p>
                Such segmentations were only previously seen in transformer-like architectures using a complex self-supervised training mechanism as in DINO, yet <b>in CRATE, segmentation emerges as a byproduct of supervised classification training</b>. In particular, the model does not obtain any <i>a priori</i> segmentation information at any time. Below, we show some example segmentations.
            </p>
            <img src="./assets/CRATE_segmentation_examples.svg" class="reg-figure"/>
            <p>
                Another remarkable property is that <b>attention heads in CRATE automatically carry semantic meaning</b>, which implies that CRATE may have post-hoc interpretability for any classification it makes. Below, we visualize the output of some attention heads across several images and several animals, showing that some attention heads correspond to different parts of the animal, and this correspondence is consistent across different animals and different classes of animals.
            </p>
            <img src="./assets/CRATE_semantic_heads.svg" class="reg-figure"/>
        </section>

        <section id="crate_ae_paper">
            <h1>Masked Autoencoding via Structured Diffusion with White-Box Transformers</h1>
            <h2>@ ICLR 2024</h2>

            <!-- Links -->
            <div class="button_list">
                <a href="https://github.com/Ma-Lab-Berkeley/CRATE" target="_blank">
                    <button>
                        <i class="ti ti-brand-github"></i>
                        GitHub
                    </button>
                </a>
                <a href="https://arxiv.org/abs/2404.02446" target="_blank">
                    <button>
                        <i class="ti ti-article"></i>
                        arXiv
                    </button>
                </a>
                <a onclick="toggleBibtex('#bibtex_iclr_paper')">
                    <button>
                        <i class="ti ti-book-2"></i>
                        BibTeX
                    </button>
                </a>
                <a href="https://colab.research.google.com/drive/1xcD-xcxprfgZuvwsRKuDroH7xMjr0Ad3?usp=sharing" target="_blank">
                    <button>
                        <i class="ti ti-pencil"></i>
                        Colab Demo
                    </button>
                </a>
            </div>

            <section id="bibtex_iclr_paper" style="display: none">
                <div style="height: 1em"></div>
                <div style="height: 1em"></div>
                <pre>
                    <code>@inproceedings{pai2024masked,
    title={Masked Completion via Structured Diffusion with White-Box Transformers},
    author={Pai, Druv and Buchanan, Sam and Wu, Ziyang and Yu, Yaodong and Ma, Yi},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024}
}</code>
                </pre>
            </section>
            <div style="height: 1em"></div>

            <h2>Autoencoding</h2>

            <p>
                We extend CRATE to <i>autoencoding</i> by using the following pipeline.
            </p>
            <img src="./assets/CRATE_ae_pipeline.svg" class="reg-figure"/>
            <p>
                To derive the decoder architecture, we propose a novel framework of <i>structured denoising-diffusion</i>, which is analogous to the common (ordinary) denoising-diffusion framework popularly used for generative modelling of imagery data. Our framework relies on a quantitative connection between the compression operator and the score function (as used in denoising-diffusion models), as shown below:
            </p>
            <img src="./assets/structured_denoising_diffusion.svg" class="narrow-figure"/>
            <p>
                The encoder and decoder are derived through discretizations of the structured denoising and diffusion processes, respectively. Importantly, the encoder derived from structured denoising is exactly the previously described CRATE architecture. The full encoder and decoder layers are given below.
            </p>
            <img src="./assets/CRATE_ae_layer.svg" class="reg-figure"/>
            <p>
                This CRATE autoencoding architecture achieves competitive performance on the masked autoencoding task, as shown by the below examples.
            </p>
            <img src="./assets/CRATE_mae_reconstruction.svg" class="reg-figure"/>
            <p>
                In addition, it obtains the same emergent properties as the classification-trained CRATE, such as interpretable attention maps in the encoder.
            </p>
            <img src="./assets/CRATE_mae_segmentation_examples.svg" class="reg-figure"/>
        </section>

        <section>
            <h1>Acknowledgements</h1>
            <p>
                This work is partially supported by the ONR grant N00014-22-1-2102, and the joint Simons Foundation-NSF DMS grant 2031899.
            </p>
            <p>
                This website template was adapted from Brent Yi's project page for <a href="https://brentyi.github.io/tilted/">TILTED</a>.
            </p>
        </section>

        <section id="follow-up-projects">
            <h1>Follow-up Projects</h1>
            <p>
                Some recent works have built off of the CRATE methodology or philosophy. Here is a non-exhaustive list of such works (with permission).
            </p>
            <ul>
                <li>"Token Statistics Transformer: Linear Time Attention via Variational Rate Reduction," which proposes a new compression objective within the CRATE framework such that the resulting Token Statistics Transformer (ToST) model has a linear-time attention mechanism which is performant as well as principled and interpretable.</li>
                <li>"Scaling White-Box Transformers for Vision" [<a href="https://rayjryang.github.io/CRATE-alpha/" target="_blank">project website</a>, <a href="https://arxiv.org/abs/2405.20299" target="_blank">paper</a>, <a href="https://github.com/UCSC-VLAA/CRATE-alpha" target="_blank">code</a>], which proposes a new sparsification mechanism within the CRATE framework such that the resulting CRATE-$\alpha$ model can scale to very large datasets and architectures while remaining mathematically and conventionally interpretable.</li>
            </ul>
        </section>
    </body>
</html>
